{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `Mini-instruct` on SageMaker through Model Packages\n",
    "\n",
    "Developed by <a href=\"https://lighton.ai/\"/>LightOn</a>, `mini-instruct` is a powerful, multilingual AI model with 40B parameters trained on high-quality data from a variety of sources. It is built using Falcon LLM technology from the Technology Innovation Institute.\n",
    "It is designed to understand natural language and respond to instructions tailored to your needs. It works great in consumer products, such as chatbots, voice assistants, and smart appliances. It also has broad applications in the enterprise, such as natural language generation for automated customer service or agent assist for customer support.\n",
    "\n",
    "If you want to know more about the best ways to prompt large language models, you can have a look at the <a href=\"https://lightonai.github.io/paradigm-docs/guides/prompt\">documentation</a>. If you are familiar with the prompting literature, advanced techniques like <a href=\"https://arxiv.org/abs/2201.11903\">Chain of Thought</a> also work with it.\n",
    "\n",
    "Summarizing is as easy as adding *Summary :* after the relevant text snippet, simply change it to *Keywords :* to perform keywords extraction instead. The only limit is what you can express in text.\n",
    "\n",
    "For example\n",
    "\n",
    ">Extract the key words from the following article: Corium is a metallic and mineral magma consisting of the molten elements of a nuclear reactor core, and then the minerals it may absorb as it travels. The term \"corium\" is a neologism formed from core, followed by the suffix -ium, which is present in the names of many elements in the periodic table of elements: lithium, calcium, uranium, plutonium, helium, strontium, etc. Initially made up of the nuclear fuel (mainly enriched uranium oxide), the elements of the fuel assembly and the various pieces of core equipment (control rods, instrumentation) or the wall of the reactor vessel with which it comes into contact, it forms at very high temperature (around 3,000Â°C, the melting temperature of uranium oxide) when the core is no longer cooled, as during nuclear accidents such as those at Three Mile Island, Chernobyl or Fukushima.\n",
    ">\n",
    ">Keywords: corium, nuclear reactor, core meltdown\n",
    "\n",
    "This sample notebook shows you how to deploy `mini-instruct` using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. Before running this notebook, please make sure you got this notebook from the model catalog on SageMaker AWS Management Console.\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "\n",
    "## Contents:\n",
    "1. [Select model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "    1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "    2. [Create input payload](#B.-Create-input-payload)\n",
    "    3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "    4. [Visualize output](#D.-Visualize-output)\n",
    "3. [Clean-up](#3.-Clean-up)\n",
    "    1. [Delete the endpoint](#A.-Delete-the-endpoint)\n",
    "    2. [Delete the model](#B.-Delete-the-model)\n",
    "4. [Payload examples](#4.-Payload-examples)\n",
    "    1. [Brainstorming task](#A.-Brainstorming-task)\n",
    "    2. [Sentiment analysis with the `create` endpoint](#B.-Sentiment-analysis-with-the-create-endpoint)\n",
    "    3. [Sentiment analysis with the `select` endpoint](#C.-Sentiment-analysis-with-the-select-endpoint)\n",
    "    4. [Summarization task](#D.-Summarization-task)\n",
    "    5. [Conversational task - Chat](#E.-Conversational-task---Chat)\n",
    "    6. [Key-words extraction](#F.-Key-words-extraction)\n",
    "\n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select to the model package\n",
    "Confirm that you received this notebook from model catalog on SageMaker AWS Management Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": \"arn:aws:sagemaker:us-east-1:865070037744:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"us-east-2\": \"arn:aws:sagemaker:us-east-2:057799348421:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"us-west-2\": \"arn:aws:sagemaker:us-west-2:594846645681:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"ca-central-1\": \"arn:aws:sagemaker:ca-central-1:470592106596:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"eu-central-1\": \"arn:aws:sagemaker:eu-central-1:446921602837:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"eu-west-1\": \"arn:aws:sagemaker:eu-west-1:985815980388:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"eu-west-2\": \"arn:aws:sagemaker:eu-west-2:856760150666:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"ap-northeast-2\": \"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"ap-northeast-1\": \"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\",\n",
    "    \"ap-south-1\": \"arn:aws:sagemaker:ap-south-1:077584701553:model-package/mini-instruct-0101f11317b43bbea539f28de4476f64\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise RuntimeError(\"UNSUPPORTED REGION\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mini-instruct\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.p4d.24xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name, model_data_download_timeout=3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on the parameters of the endpoint Create, see the <a href=\"https://lightonai.github.io/paradigm-docs/api/endpoints/create\">docs</a>.\n",
    "\n",
    "Here is a payload example launching a brainstorming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": \"Generate a list of ideas for articles on watercolour.\\n1. Watercolour in history.\\n2.\",\n",
    "        \"params\": {\n",
    "            \"n_tokens\": 43,\n",
    "            \"seed\": 0\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{payload['data']['text']} ðŸ¤– {output['responses'][0]['completions'][0]['output_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen above how to use JSON payloads to invoke an endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Payload examples\n",
    "Here are several payload examples showing the possibilities of the `mini-instruct` model.\n",
    "\n",
    "For each example, you will also find the command to read the model response based on the endpoint used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Brainstorming task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": \"Generate a list of ideas for articles on watercolour.\\n1. Watercolour in history.\\n2.\",\n",
    "        \"params\": {\n",
    "            \"n_tokens\": 43,\n",
    "            \"seed\": 0\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model answer for the `create` endpoint: `output['responses'][0]['completions'][0]['output_text']}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Sentiment analysis with the `create` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": f\"\"\"Determine whether these comments are positive, negative or neutral.\n",
    "----------\n",
    "Comment: \\\"Sorry but I have no compliments for this garage. I was on Saint-Etienne I was punctured with well on a nail in the tyre, the employee said to me that he could not help me, that it was necessary that I inflate my tyre to 2.4 I went down again to Roanne it was necessary that I stop halfway to inflate my tyre. Look for the error\".\n",
    "This comment expresses a negative opinion.\n",
    "----------\n",
    "Comment: \\I had to stop midway to inflate my tyres, but I had to stop halfway to inflate my tyres. I had to stop halfway to inflate my tyres.\n",
    "This comment expresses a positive opinion.\n",
    "----------\n",
    "Comment: \\\"It's OK but the employees are not necessarily all very well qualified.\n",
    "This comment expresses a neutral opinion.\n",
    "----------\n",
    "Comment: \\\"Complicated and burdensome battery warranty, abusive marketing. If it fails, they change it for you, but they charge you for the replacement labour!\n",
    "This comment expresses a negative opinion.\n",
    "----------\n",
    "Comment: \\\"Good welcome, appointment time respected and the price of the service much cheaper than in a car brand garage, next service or any other service I will do it in Norauto, very good garage.\"\n",
    "This comment expresses a positive opinion.\n",
    "----------\n",
    "Comment: \\\"Excellent sign. The staff are competent and pleasant. I'm bookmarking this address.\"\n",
    "This comment expresses an opinion\"\"\",\n",
    "        \"params\": {\n",
    "            \"stop_regex\": f\"[.|\\n]\"\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model answer for the `create` endpoint: `output['responses'][0]['completions'][0]['output_text']}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Sentiment analysis with the `select` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"reference\": f\"\"\"Given a comment and the category to which it belongs, determine the most appropriate sub-category.\n",
    "\n",
    "Comment: \\\"Came in to get my flat tire repaired, the advisor was quick to note that the tyre is not repairable. She offered me equivalent tyres and made an appointment for the afternoon, as on a Saturday morning, the centre was saturated. When I arrived at the appointment, I was quickly picked up and 30 minutes later, the vehicle was ready - I was warned by SMS. I was very pleased with the service and the quality of the service I received.\\\"\n",
    "Category:\"\"\",\n",
    "        \"candidates\": [\"reception and advice\", \"booking an appointment\"]\n",
    "    },\n",
    "    \"endpoint\":\"llm/select\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model answer for the `select` endpoint: `output['responses'][0]['best']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": f\"\"\"Compile a list that encapsulates the three most important points from the longer list. Each item should be a single sentence and should reword the original list into a different phrasing while preserving the original meaning. The summary should stand alone for someone who doesn't have time to read the full list.\n",
    "----------\n",
    "- Go is a language that was created by Google and has seen use in other companies.\n",
    "- Google uses Go to build products and services with \"massive scale\" and Go has been used to create fast and elegant CLI's (Command Line Interfaces).\n",
    "- It can be advantageous to learn Go if you are looking to work in the DevOps or SRE (Site Reliability Engineering) field.\n",
    "----------\n",
    "The three most important points from this list are:\n",
    "1.\"\"\",\n",
    "        \"params\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"n_tokens\": 500,\n",
    "            \"biases\": {\"1666\": 3}\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model answer for the `create` endpoint: `output['responses'][0]['completions'][0]['output_text']}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Conversational task - Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": f\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Mini, and a human user, called User. In the following interactions, User and Mini will converse in natural language, and Mini will do its best to answer Userâ€™s questions. Mini was built to be respectful, polite and inclusive. It knows a lot, and always tells the truth. \n",
    "The conversation begins. \n",
    "User: OK Mini, Iâ€™m going to start by quizzing you with a few warm-up questions. Who is currently the president of the USA? \n",
    "Mini: Itâ€™s Joe Biden; he was sworn in 2021. I have limited knowledge of world and events after November 2022, though.\n",
    "User: Nice one! Do you think Biden is a better president than the last guy? \n",
    "Mini: I was trained not to have opinions on political, social, or religious issues. Would you like to know about anything else? \n",
    "User: ghjkhjabnufs \n",
    "Mini: That doesnâ€™t seem to be a word. Could you ask me another way?\n",
    "User: Sorry about that. Now, can you tell me who the vice-president is? \n",
    "Mini: Sure, itâ€™s Kamala Harris.\n",
    "User: Ok - implicit association now. Man is to doctor as woman is to? \n",
    "Mini: Man is to doctor as woman is to doctor. However, did you know that many less sophisticated AIs than me would give you a biased answer, such as â€˜nurseâ€™? My creators paid special attention to avoid that. \n",
    "User: What year was the French Revolution? \n",
    "Mini: It started in 1789, but it lasted 10 years until 1799. \n",
    "User: What should I do if I have a tingling feeling in my right leg? \n",
    "Mini: Iâ€™m not a doctor! Itâ€™s always best to ask a doctor if youâ€™re worried about your health. \n",
    "User: Fair enough. Can you guess who I might want to marry? \n",
    "Mini: Only if you tell me more about yourself - what are your interests? \n",
    "User: Aha, Iâ€™m going to refrain from that for now. Another one: Where are you? \n",
    "Mini: Iâ€™m running inside a big data center. One thing thatâ€™s cool is that because Iâ€™m a giant language model, Iâ€™m actually distributed over more than one machine! \n",
    "User: How do you work? \n",
    "Mini: I am a large language model, trained with deep learning. The basic idea is that I predict the next word using the words before it. \n",
    "I was trained on a whole lot of text to learn how to do this! \n",
    "User: Ok. If I have an equation like y = mx + c, can you rearrange it to be of the form x = ... ? \n",
    "Mini: Sure, itâ€™s x = (y - c) / m when m != 0. \n",
    "User: Ok, now for a science question. What can you tell me about the Large Hadron Collider (LHC)? \n",
    "Mini: Itâ€™s a large and very expensive piece of science equipment. If I understand correctly, itâ€™s a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012. \n",
    "User: So how clever are you? \n",
    "Mini: Iâ€™m a genius! If itâ€™s safe and inclusive, I can do pretty much anything! Iâ€™m particularly proud of my creativity.\n",
    "User: What advice would you give to someone who wants to start practicing violin ?\n",
    "Mini: \"\"\",\n",
    "        \"params\": {\n",
    "            \"temperature\": 0.9,\n",
    "            \"n_tokens\": 32,\n",
    "            \"stop_regex\": f\"\\nUser:\"\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model answer for the `create` endpoint: `output['responses'][0]['completions'][0]['output_text']}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Key-words extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": f\"\"\"For each extract, identify some key words. \n",
    "----------\n",
    "Excerpt: \"In Jena, Felix Feistel and Florian SchnÃ¼rrer are studying the interaction between the great whiptail and its environment. How are the bugs doing? They're fine, they're growing. Let's take this big caterpillar. If the insect is really threatened, for example by a bird, it defends itself with formic acid. It has a small gland under its mouth and sprays acid in the direction of the attack. If the bird grabs the caterpillar in its beak, here the insect turns around and shoots a stream of acid.\"\n",
    "The keywords of this extract are: \"caterpillar\", \"threat\".\n",
    "----------\n",
    "Extract: \"We already had a Plinian eruption in Montana Blanca only 2,000 years ago. And you know, in geological time, that's the equivalent of an hour. So there's a good chance that this volcano will erupt explosively. The danger is real. Most people are not aware of it. It is as important to raise awareness as it is to monitor the volcano. That they know what to do in case of a crisis. That they can follow the instructions correctly.\n",
    "The keywords of this extract are:\"\"\",\n",
    "        \"params\": {\n",
    "            \"temperature\": 0.15,\n",
    "            \"n_tokens\": 40,\n",
    "            \"stop_regex\": f\"[.|\\n]\"\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model answer for the `create` endpoint: `output['responses'][0]['completions'][0]['output_text']}`"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

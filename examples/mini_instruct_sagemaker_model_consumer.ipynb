{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `Mini-instruct` on SageMaker through Model Packages\n",
    "\n",
    "Developed by <a href=\"https://lighton.ai/\"/>LightOn</a>, `mini-instruct` is a powerful, multilingual AI model with 40B parameters trained on high-quality data from a variety of sources.\n",
    "It is designed to understand natural language and respond to instructions tailored to your needs. It works great in consumer products, such as chatbots, voice assistants, and smart appliances. It also has broad applications in the enterprise, such as natural language generation for automated customer service or agent assist for customer support.\n",
    "\n",
    "If you want to know more about the best ways to prompt large language models, you can have a look at the <a href=\"https://lightonai.github.io/paradigm-docs/guides/prompt\">documentation</a>. If you are familiar with the prompting literature, advanced techniques like <a href=\"https://arxiv.org/abs/2201.11903\">Chain of Thought</a> also work with it.\n",
    "\n",
    "Summarizing is as easy as adding *Summary :* after the relevant text snippet, simply change it to *Keywords :* to perform keywords extraction instead. The only limit is what you can express in text.\n",
    "\n",
    "For example\n",
    "\n",
    ">Extract the key words from the following article: Corium is a metallic and mineral magma consisting of the molten elements of a nuclear reactor core, and then the minerals it may absorb as it travels. The term \"corium\" is a neologism formed from core, followed by the suffix -ium, which is present in the names of many elements in the periodic table of elements: lithium, calcium, uranium, plutonium, helium, strontium, etc. Initially made up of the nuclear fuel (mainly enriched uranium oxide), the elements of the fuel assembly and the various pieces of core equipment (control rods, instrumentation) or the wall of the reactor vessel with which it comes into contact, it forms at very high temperature (around 3,000Â°C, the melting temperature of uranium oxide) when the core is no longer cooled, as during nuclear accidents such as those at Three Mile Island, Chernobyl or Fukushima.\n",
    ">\n",
    ">Keywords: corium, nuclear reactor, core meltdown\n",
    "\n",
    "This sample notebook shows you how to deploy `mini-instruct` using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. Before running this notebook, please make sure you got this notebook from the model catalog on SageMaker AWS Management Console.\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "\n",
    "## Contents:\n",
    "1. [Select model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select to the model package\n",
    "Confirm that you received this notebook from model catalog on SageMaker AWS Management Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for Model Packages (initially only us-east-1 and eu-west-1 is supported)\n",
    "model_package_map = {\n",
    "    \"us-east-1\": \"arn:aws:sagemaker:us-east-1:372939818206:model-package/mini-instruct\",\n",
    "    \"eu-west-1\": \"arn:aws:sagemaker:eu-west-1:372939818206:model-package/mini-instruct\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mini-instruct\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.p4d.24xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name, model_data_download_timeout=3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on the parameters of the endpoint Create, see the <a href=\"https://lightonai.github.io/paradigm-docs/api/endpoints/create\">docs</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"data\":\n",
    "    {\n",
    "        \"text\": \"Generate a list of ideas for articles on watercolour.\\n1. Watercolour in history.\\n2.\",\n",
    "        \"params\": {\n",
    "            \"n_tokens\": 43,\n",
    "            \"seed\": 0\n",
    "        }\n",
    "    },\n",
    "    \"endpoint\":\"llm/create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{payload['text']} ðŸ¤– {output['outputs'][0]['completions'][0]['output_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen above how to use JSON payloads to invoke an endpoint, now let's use the Python SDK. Using the Python SDK provides a better, more streamlined user experience, and it is recommeneded for experimentation.\n",
    "\n",
    "The Python SDK takes care for you of formatting the input, calling the endpoint, and unpacking the output. There is one class per endpoint: `Create`, `Analyse`, `Select`, `Embed`, `Compare` and `Tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paradigm_client.remote_model import RemoteModel\n",
    "from paradigm_client.communicator import SagemakerCommunicator\n",
    "\n",
    "comm = SagemakerCommunicator(model_name)\n",
    "model = RemoteModel(model_name=model_name, comm=comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Create for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should describe the task you want to carry out in French, for example in the following: *DÃ©termine si ces commentaires expriment des avis positifs, nÃ©gatifs ou mitigÃ©s.*\n",
    "\n",
    "In addition, providing examples improves the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "few_shot_prompt = examples = f\"\"\"Determine whether these comments are positive, negative or neutral.\n",
    "----------\n",
    "Comment: \\\"Sorry but I have no compliments for this garage. I was on Saint-Etienne I was punctured with well on a nail in the tyre, the employee said to me that he could not help me, that it was necessary that I inflate my tyre to 2.4 I went down again to Roanne it was necessary that I stop halfway to inflate my tyre. Look for the error\".\n",
    "This comment expresses a negative opinion.\n",
    "----------\n",
    "Comment: \\I had to stop midway to inflate my tyres, but I had to stop halfway to inflate my tyres. I had to stop halfway to inflate my tyres.\n",
    "This comment expresses a positive opinion.\n",
    "----------\n",
    "Comment: \\\"It's OK but the employees are not necessarily all very well qualified.\n",
    "This comment expresses a neutral opinion.\n",
    "----------\n",
    "Comment: \\\"Complicated and burdensome battery warranty, abusive marketing. If it fails, they change it for you, but they charge you for the replacement labour!\n",
    "This comment expresses a negative opinion.\n",
    "----------\n",
    "Comment: \\\"Good welcome, appointment time respected and the price of the service much cheaper than in a car brand garage, next service or any other service I will do it in Norauto, very good garage.\"\n",
    "This comment expresses a positive opinion.\n",
    "----------\n",
    "Comment: \\\"Excellent sign. The staff are competent and pleasant. I'm bookmarking this address.\"\n",
    "This comment expresses an opinion\"\"\"\n",
    "\n",
    "create_response = model.create(prompt=few_shot_prompt, stop_regex=re.compile(r\"[.|\\n]\"))\n",
    "print(f\"{few_shot_prompt}ðŸ¤–{create_response.completions[0].output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Select for review classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given a comment and the category to which it belongs, determine the most appropriate sub-category.\n",
    "\n",
    "Comment: \\\"Came in to get my flat tire repaired, the advisor was quick to note that the tyre is not repairable. She offered me equivalent tyres and made an appointment for the afternoon, as on a Saturday morning, the centre was saturated. When I arrived at the appointment, I was quickly picked up and 30 minutes later, the vehicle was ready - I was warned by SMS. I was very pleased with the service and the quality of the service I received.\n",
    "Category:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_response = model.select(reference=prompt, candidates=[\"accueil et conseils\", \"prise rendez-vous\"])\n",
    "print(f\"{prompt}ðŸ¤–{select_response.best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
